import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error
from sklearn.datasets import make_regression
from sklearn.linear_model import Ridge,RidgeCV,Lasso,lasso_path,enet_path
from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler








X, y, w = make_regression(n_samples =100, n_features =10, n_informative =8,
coef=True , random_state =42)
w





lambdas = np.logspace(-3,4,100)


coefs = []
errors_coefs = []
for a in lambdas:
    clf = Ridge(alpha=a)
    clf.fit(X, y)
    coefs.append(clf.coef_)
    errors_coefs.append(mean_absolute_error(clf.coef_ , w))


for i in range(len(coefs[0])): 
    plt.plot(lambdas, np.array(coefs)[:,i], label=f"Coef {i+1}")
plt.xscale("log")
plt.xlabel("Lambdas")
plt.ylabel("Coefficients")
plt.title("Ã‰volution des coefficients en fonction des lambdas")
plt.legend() 
plt.grid(True)
plt.show()






plt.plot(lambdas,errors_coefs)
plt.xlabel("lambdas")
plt.title('Erreur absolue moyenne')
plt.ylabel("erreur absolue moyenne")
plt.xscale('log')








df = pd.read_csv("./real_estate.csv", delimiter=",")
df.drop(columns =["No"], inplace=True)
y = df["Y house price of unit area"]
X = df.drop(columns =["Y house price of unit area"])
df.head(10)


df.info()





X_train,X_Test,y_train,y_Test=train_test_split(X,y,test_size=0.5,random_state=42)





ridge = RidgeCV(alphas=lambdas).fit(X_train, y_train)
pred = ridge.preridge = RidgeCV(alphas=lambdas).fit(X_train, y_train)
pred = ridge.predict (X_Test)
print(f"Best lambda is {ridge.alpha_}")





MSE=mean_squared_error(y_Test,pred)
r2=r2_score(y_Test,pred)
print(f'MSE = {MSE} et R2 = {r2}')





scaler = StandardScaler()
X_train_std = scaler.fit_transform(X_train)
X_test_std = scaler.transform(X_Test)





ridge = RidgeCV(alphas=lambdas).fit(X_train_std, y_train)
pred = ridge.predict (X_test_std)
print(f"Best lambda is {ridge.alpha_}")





MSE=mean_squared_error(y_Test,pred)
r2=r2_score(y_Test,pred)
print(f'MSE = {MSE} et R2 = {r2}')














df = pd.read_csv("./baseball_processed.csv", delimiter=",")
print(df.describe())


df.head(5)





target=df['lnSalary']
data=df.drop(columns='lnSalary')
X_train,X_Test,Y_Train,Y_Test=train_test_split(data,target,test_size=0.5,random_state=42)





scaler = StandardScaler()
X_train_std = scaler.fit_transform(X_train)
X_test_std = scaler.transform(X_Test)





lasso=Lasso()
lasso.fit(X_train,Y_Train)
print(lasso.coef_)





pass





lambdas = np.logspace(-4,1,100)





alphas_lasso , coefs_lasso , _ = lasso_path(X_train_std , Y_Train ,alphas=lambdas)











fig , axs = plt.subplots (1,2, figsize =(25, 10)) # change the size if needed
# 16 different colors , one per coefficient:
colors = ["blue", "red", "green", "orange", "purple", "pink", "brown", "yellow",
"cyan", "magenta", "olive", "lime", "teal", "coral", "navy", "indigo", "gold"]
i=0
for coef_l , c in zip(coefs_lasso , colors):
    axs[0]. plot(alphas_lasso , coef_l , color=c,label=f'coef {i+1}')
    axs[0]. set_xscale('log')
    i+=1
axs[0]. set_xscale('log')
axs[0].set_xlabel('lambda ')
axs[0].set_ylabel('Coefficients ')
axs[0].set_title('Lasso path')
axs[0].legend()
axs[1].legend()











alphas_enet , coefs_enet , _ = enet_path(X_train_std , Y_Train ,alphas=lambdas)


fig , axs = plt.subplots (1,2, figsize =(25, 10)) # change the size if needed
# 16 different colors , one per coefficient:
colors = ["blue", "red", "green", "orange", "purple", "pink", "brown", "yellow",
"cyan", "magenta", "olive", "lime", "teal", "coral", "navy", "indigo", "gold"]
i=0
for coef_l , c in zip(coefs_enet , colors):
    axs[0]. plot(alphas_enet , coef_l , color=c,label=f'coef {i+1}')
    axs[0]. set_xscale('log')
    i+=1
axs[0].set_xlabel('lambda ')
axs[0].set_ylabel('Coefficients ')
axs[0].set_title('Lasso path')
axs[0].legend()





fig , axs = plt.subplots (1,2, figsize =(25, 10)) # change the size if needed
# 16 different colors , one per coefficient:
colors = ["blue", "red", "green", "orange", "purple", "pink", "brown", "yellow",
"cyan", "magenta", "olive", "lime", "teal", "coral", "navy", "indigo", "gold"]
i=0
for coef_l , c in zip(coefs_enet , colors):
    axs[0]. plot(alphas_enet , coef_l , color=c,label=f'coef {i+1}')
    axs[0]. set_xscale('log')
    i+=1
for i in range(len(coefs_lasso)): 
    axs[1].plot(alphas_enet, coefs_lasso[i],'o-',label=f"Coef_lasso {i+1}")
    axs[1].plot(alphas_enet, coefs_enet[i], label=f"Coef_enet {i+1}")
axs[0].set_xlabel('lambda ')
axs[1].set_xscale('log')
axs[0].set_ylabel('Coefficients ')
axs[0].set_title('Elastic-Net path')

axs[0].legend()
axs[1].legend()








l1=[0.01,0.1,0.3,0.5,0.6,0.9]
plt.subplots (2,3, figsize =(25, 10))
plt.suptitle('Importance de la valeur de l1_ration')
for l in range(len(l1)):
    alphas_enet , coefs_enet , _ = enet_path(X_train_std , Y_Train ,alphas=lambdas,l1_ratio=l1[l])
    # 16 different colors , one per coefficient:
    colors = ["blue", "red", "green", "orange", "purple", "pink", "brown", "yellow",
    "cyan", "magenta", "olive", "lime", "teal", "coral", "navy", "indigo", "gold"]
    i=0
    plt.subplot(2,3,l+1)
    plt.title(f'l1_ratio = {l1[l]}')
    for coef_l , c in zip(coefs_enet , colors):
        plt. plot(alphas_enet , coef_l , color=c,label=f'coef {i+1}')
        plt.xscale('log')
        i+=1
    plt.xlabel('lambda ')
    plt.ylabel('Coefficients ')



