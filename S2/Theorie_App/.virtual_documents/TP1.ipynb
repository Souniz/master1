import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')


path = "./winequality-white.csv"
df = pd.read_csv(path , header="infer", delimiter=";")
print("\n========= Dataset summary ========= \n")
print(df.info())
print("\n========= First instances ========= \n")
print(df.head(5))
print(f'Nombre intance {len(df)}')
print(f'Nombre de caractéristiques {len(df.columns)}')



X = df.drop("quality", axis =1) 
Y = df["quality"]
print("\n========= Wine Qualities ========= \n")
print(Y.unique())
print("\n========= Number of instance per qualities ========= \n")
df.groupby('quality')['pH'].count()





plt.figure ()
sb.boxplot(data=X,orient="v",palette="Set1",width =1.5, notch=True)
ax = plt.gca()
ax.set_xticklabels(ax.get_xticklabels (),rotation =90)
plt.figure ()
corr = X.corr()
sb.heatmap(corr)


X.describe()














s=[0 if i<5 else 1 for i in Y]
Y=s


data_train,X_Test,label_train,Y_Test=train_test_split(X,Y,test_size=0.2,random_state=42,stratify=Y)
X_train,X_valid,Y_train,Y_valid=train_test_split(data_train,label_train,test_size=0.2,random_state=42,stratify=label_train)








clf=KNeighborsClassifier(n_neighbors=3)
clf.fit(X_train,Y_train)
y_predi=clf.predict(X_valid)
taux_errer=sum([1 for i in range(len(y_predi)) if Y_valid[i]!=y_predi[i]])/len(X_valid)
print(f"\n========= Taux d'erreur {taux_errer}========= \n")








taux_erreur_train=[]
taux_erreur_validation=[]
for k in range(1,40):
    clf=KNeighborsClassifier(n_neighbors=k)
    clf.fit(X_train,Y_train)
    y_predi_train=clf.predict(X_train)
    y_predi_val=clf.predict(X_valid)
    erreur_train=sum([1 for i in range(len(y_predi_train)) if Y_train[i]!=y_predi_train[i]])/len(Y_train)
    erreur_valid=sum([1 for i in range(len(y_predi_val)) if Y_valid[i]!=y_predi_val[i]])/len(Y_valid)
    taux_erreur_train.append(erreur_train)
    taux_erreur_validation.append(erreur_valid)
plt.figure(figsize=(12,6))
plt.plot([i for i in range(1,40)],taux_erreur_train,label='Erreur en apprentissage')
plt.plot([i for i in range(1,40)],taux_erreur_validation,label='Erreur en validation ')
plt.legend()
plt.show()











#Fusionne les données d'entrainement et de validation
data_train=pd.concat([X_train,X_valid])
data_labels=np.concat((np.array(Y_train),np.array(Y_valid)))
clf=KNeighborsClassifier(n_neighbors=17)
clf.fit(data_train,data_labels)
y_predi=clf.predict(X_Test)
taux_errer=sum([1 for i in range(len(X_Test)) if Y_Test[i]!=y_predi[i]])/len(X_Test)
print(f"\n========= Taux d'erreur {taux_errer}========= \n")








taux_erreur_train=[]
taux_erreur_validation=[]
sc = StandardScaler(with_mean=True , with_std=True)
sc = sc.fit(X_train)
Xa_n = sc.transform(X_train)
Xv_n = sc.transform(X_valid)
for k in range(1,40):
    clf=KNeighborsClassifier(n_neighbors=k)
    clf.fit(Xa_n,Y_train)
    y_predi_train=clf.predict(Xa_n)
    y_predi_val=clf.predict(Xv_n)
    erreur_train=sum([1 for i in range(len(y_predi_train)) if Y_train[i]!=y_predi_train[i]])/len(Y_train)
    erreur_valid=sum([1 for i in range(len(y_predi_val)) if Y_valid[i]!=y_predi_val[i]])/len(Y_valid)
    taux_erreur_train.append(erreur_train)
    taux_erreur_validation.append(erreur_valid)
plt.figure(figsize=(12,6))
plt.plot([i for i in range(1,40)],taux_erreur_train,label='Erreur en apprentissage')
plt.plot([i for i in range(1,40)],taux_erreur_validation,label='Erreur en validation ')
plt.legend()
plt.show()








#Fusionne les données d'entrainement et de validation
data_train=pd.concat([X_train,X_valid])
X_n=sc.transform(data_train)
data_labels=np.concat((np.array(Y_train),np.array(Y_valid)))
clf=KNeighborsClassifier(n_neighbors=15)
clf.fit(data_train,data_labels)
Xt_n=sc.transform(X_Test)
y_predi=clf.predict(Xt_n)
taux_errer=sum([1 for i in range(len(X_Test)) if Y_Test[i]!=y_predi[i]])/len(Y_Test)
print(f"\n========= Taux d'erreur {taux_errer}========= \n")



