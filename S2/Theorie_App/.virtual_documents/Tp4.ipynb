import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler





X1=np.random.normal((1,1),0.7,size=(300,2))
X2=np.random.normal((-1,-1),0.7,size=(300,2))





plt.scatter(X1[:,0],X1[:,1])
plt.scatter(X2[:,0],X2[:,1])





y=np.array([1 if i <300 else 0 for i in range(600)])
X=np.concatenate((X1,X2),axis=0)


def mesh(X, h=0.02):
    x_min , x_max = X[:, 0]. min() - .5, X[:, 0]. max() + .5
    y_min , y_max = X[:, 1]. min() - .5, X[:, 1]. max() + .5
    xx , yy = np.meshgrid(np.arange(x_min , x_max , h), np.arange(y_min , y_max , h))
    return np.c_[xx.ravel (), yy.ravel ()], xx , yy
    
xtest_grid , xx , yy = mesh(X)
log_reg=LogisticRegression()
log_reg.fit(X,y)
Z = log_reg.predict_proba(xtest_grid)[:, 0]
Z = Z.reshape(xx.shape)
plt.pcolormesh(xx , yy , Z, cmap=plt.cm.RdBu , alpha =.5)
plt.scatter(X[y == 0, 0], X[y == 0, 1], c='b', edgecolors="k", s=20)
plt.scatter(X[y == 1, 0], X[y == 1, 1], c='r', edgecolors="k", s=20)
plt.show()





class my_Logsitic:
    def __init__(self,backtrak=False):
        self.coef=[]
        self.backtrak=backtrak
        self.list_cost=[]

    def train(self,X,y,):
        self.coef,self.list_cost=self.logreg_descent(X, y)

    def costs(self):
        return self.list_cost
    
    #Utilisation de la methode de backtrak pour le pas optimal
    def step_backtrak(self,X,y,w,direction,step,beta):
        c=1e-4
        f_x=self.logreg_cost(X,y,w)
        while(self.logreg_cost(X,y,w+step*direction)>f_x-c*step*direction@direction) :
            step=step*beta
        return step
    
    # 1. Implémentation de la fonction qui calcule du coût 
    def logreg_grad(self,X, y, w):
        z=X@w
        p=np.exp(z)/(1+np.exp(z))
        grad=-X.T@(y-p)
        return grad

    #1. Implémentation de la fonction qui calcule le gradien 
    def logreg_cost(self,X, y, w):
        z=X@w
        cost=-(y@z)+np.sum(np.log(1+np.exp(z)))
        return cost
    
    # L’estimation des paramètres du modèle de regression logistique avec la méthode de descente de gradient
    def logreg_descent(self,X, y, learning_rate =1e-3, max_iter =1000):
        n, d = X.shape
        X = np.concatenate ((X, np.ones((n, 1))), axis =1)
        w0 = np.random.randn(d+1)
        step = learning_rate
        beta = 0.9 # for backtracking if needed - to tune
        cost=[]
        iteration=0
        if self.backtrak==True:
           step=self.step_backtrak(X,y,w0,-self.logreg_grad(X, y, w0),1e-3,beta)
        w=w0-step*self.logreg_grad(X, y, w0)
        cost.append(self.logreg_cost(X,y,w))
        while np.linalg.norm(self.logreg_grad(X, y, w))>1e-6 and iteration<max_iter:
            if self.backtrak==True:
                step=self.step_backtrak(X,y,w,-self.logreg_grad(X, y, w),1e-3,beta)
            w=w-step*self.logreg_grad(X, y, w)
            iteration+=1
            cost.append(self.logreg_cost(X,y,w))
        
        return w[:-1],cost
    
    # La fonction qui calcule les probas à posteori des classes
    def calcul_proba(self,x):
        z=x@self.coef
        if len(x.shape)==1:
            p=np.exp(z)/(1+np.exp(z))
            return np.array([p,1-p])
        else:
            p=np.exp(z)/(1+np.exp(z))
            return np.array([[1-i,i] for i in p])
        
    #Predire les classe
    def predict(self,x):
       proba=self.calcul_proba(x)
       return np.array([np.argmax(i) for i in proba])
    
    # Calcul du score
    def score(self,x,y):
        pred=self.predict(x)
        taux=np.array([1 if pred[i]==y[i] else 0 for i in range(len(y))])
        print(f'Taux de bonne classification = {np.round(taux.mean() *100,3)}% Ecart-type = {np.round(taux.std(),3)}')






xtest_grid , xx , yy = mesh(X)
clf=my_Logsitic(backtrak=True)
clf.train(X,y)
Z = clf.calcul_proba(xtest_grid)[:,0]
Z = Z.reshape(xx.shape)
plt.pcolormesh(xx , yy , Z, cmap=plt.cm.RdBu , alpha =.5)
plt.scatter(X[y == 0, 0], X[y == 0, 1], c='b', edgecolors="k", s=20)
plt.scatter(X[y == 1, 0], X[y == 1, 1], c='r', edgecolors="k", s=20)
plt.show()


plt.plot(clf.list_cost,'-')
plt.title(f'Parametre {clf.coef}')
plt.xlabel('Nombre Iteration')
plt.ylabel('Evolution du cout')
plt.grid()








X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.3,stratify=y)


clf=my_Logsitic(backtrak=True)
clf.train(X_train, Y_train)


xtest_grid , xx , yy = mesh(X_test)
Z = clf.calcul_proba(xtest_grid)[:,0]
Z = Z.reshape(xx.shape)
plt.pcolormesh(xx , yy , Z, cmap=plt.cm.RdBu , alpha =.5)
plt.scatter(X_test[Y_test == 0, 0], X_test[Y_test == 0, 1], c='b', edgecolors="k", s=20)
plt.scatter(X_test[Y_test == 1, 0], X_test[Y_test == 1, 1], c='r', edgecolors="k", s=20)
plt.show()








data=load_breast_cancer()
target=data.target
data=data.data
X_train,X_test,Y_train,Y_test=train_test_split(data,target,test_size=0.3,stratify=target)
std=StandardScaler()
X_train=std.fit_transform(X_train)
X_test=std.transform(X_test)





clf=LogisticRegression()
clf.fit(X_train,Y_train)
classe_predi=clf.predict(X_test)
taux=np.array([1 if classe_predi[i]==Y_test[i] else 0 for i in range(len(Y_test))])
print(f'Taux de bonne classification = {np.round(taux.mean() *100,3)}% Ecart-type = {np.round(taux.std(),3)}')


clf=my_Logsitic()
clf.train(X_train,Y_train)
socre=clf.score(X_test,Y_test)


clf=my_Logsitic()
clf.train(X_train,Y_train)
socre=clf.score(X_test,Y_test)





#Version backtark
clf=my_Logsitic(backtrak=True)
clf.train(X_train,Y_train)
socre=clf.score(X_test,Y_test)





clf=LogisticRegression(penalty='l2',C=0.1)
clf.fit(X_train,Y_train)
classe_predi=clf.predict(X_test)
taux=np.array([1 if classe_predi[i]==Y_test[i] else 0 for i in range(len(Y_test))])
print(f'Taux de bonne classification = {np.round(taux.mean() *100,3)}% Ecart-type = {np.round(taux.std(),3)}')



