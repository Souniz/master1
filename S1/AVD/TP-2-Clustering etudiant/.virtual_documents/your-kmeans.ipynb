


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# M1 Science et Ingénieurie des données
# Université de Rouen Normandie
# T. Paquet
import matplotlib.pyplot as plt
from sklearn import datasets
import numpy as np
from numpy.linalg import norm
import sklearn
colors =['r','b','g','c','m','o']
n_colors = 6


def my_kmeans(X,K,Visualisation=False,Seuil=0.001,Max_iterations = 100000):
    
    N,p = np.shape(X)
    iteration = 0        
    Dist=np.zeros((K,N))
    J=np.zeros(Max_iterations+1)
    J[0] = 10000000
    
    # Initialisation des clusters
    # par tirage de K exemples, pour tomber dans les données     
 
    Index_init = np.random.choice(N, K,replace = False)
    C = np.zeros((p,K))
    for k in range(K):
        C[:,k] = X[Index_init[k],:].T 
        
        
    while iteration < Max_iterations:
        iteration +=1
        #################################################################
        # E step : estimation des données manquantes 
        #          affectation des données aux clusters les plus proches
        for k in range(K):
            Dist[k,:]=np.linalg.norm(X-C[:,k],axis=1)**2
        y=np.argmin(Dist,axis=0)
        #################################################################
        # M Step : calcul des meilleurs centres          
        for k in range(K):
              C[:,k]=np.mean(X[y==k,:],axis=0)
        #################################################################
        # test du critère d'arrêt l'évolution du critère est inférieure 
        # au Seuil en pour ceent
        J[iteration]=np.sum(np.min(Dist[y,:],axis=0))/N

        if np.abs(J[iteration]-J[iteration-1])/J[iteration-1]<Seuil:
             break;
    return C, y,J[1:iteration]





#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# M1 Science et Ingénieurie des données
# Université de Rouen Normandie
# T. Paquet
import matplotlib
import matplotlib.pyplot as plt
from sklearn import datasets
import numpy as np
from numpy.linalg import norm

###########################################################################
# initialisation ++ du kmeans
#
def initPlusPlus(X,K):
    N,p = np.shape(X)
    C = np.zeros((p,K))
    generator = np.random.default_rng()
    index = np.random.choice(N, 1,replace = False)
    liste_index = [index]
    C[:,0] = X[index,:]
    X = np.delete(X,index,0)
    # print("k=0 C[k]=",C[:,0],"index=",index)
    k=1
    
    while k < K:
        
        # calcul des distances
        NN = X.shape[0]
        dist = np.zeros(NN)
        for n in range(NN):
            D=C[:,:k]-np.repeat(X[n,:],k).reshape(p,k)
            D=np.diag(D@D.T)
            dist[n]=np.min(D)
       # ICI ..... 
        # calcul des probabilités
        proba=dist/np.sum(dist)
        range_value=generator.random((1))[0]
        intervals=np.cumsum(proba)
        index=0
        while index<NN:
            if intervals[index]>range_value:
                break;
            index+=1
        # ICI ....
        
        # tirage aléatoire selon proba
        C[:,k]=X[index,:]
        X=np.delete(X,index,0)
        k+=1
       # ICI ...........
    return C
#----------------K-mean++-------------------------------
def my_kmeans_plus_plus(X,K,Visualisation=False,Seuil=0.001,Max_iterations = 1000):
    
    N,p = np.shape(X)
    iteration = 0        
    Dist=np.zeros((K,N))
    J=np.zeros(Max_iterations+1)
    J[0] = 10000000
    C=initPlusPlus(X,K)
        
    while iteration < Max_iterations:
        iteration +=1
        #################################################################
        # E step : estimation des données manquantes 
        #          affectation des données aux clusters les plus proches
        for k in range(K):
            Dist[k,:]=np.linalg.norm(X-C[:,k],axis=1)**2
        y=np.argmin(Dist,axis=0)
        #################################################################
        # M Step : calcul des meilleurs centres          
        for k in range(K):
              C[:,k]=np.mean(X[y==k,:],axis=0)
        #################################################################
        # test du critère d'arrêt l'évolution du critère est inférieure 
        # au Seuil en pour ceent
        J[iteration]=np.sum(np.min(Dist[y,:],axis=0))/N

        if np.abs(J[iteration]-J[iteration-1])/J[iteration-1]<Seuil:
             break;
    return C, y,J[1:iteration]





#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# M1 Science et Ingénieurie des données
# Université de Rouen Normandie
# T. Paquet
import matplotlib
import matplotlib.pyplot as plt
from sklearn import datasets
import numpy as np
from numpy.linalg import norm

colors =['r','b','g','c','m','o']
n_colors = 6


def my_kmean_medoide(X,K,Visualisation=False,Seuil=0.0001,Max_iterations = 1000):
    
    N,p = np.shape(X)
    iteration = 0        
    Dist=np.zeros((K,N))
    J=np.zeros(Max_iterations+1)
    J[0] = 10000000
    
    # Initialisation des clusters
    # par tirage de K exemples, pour tomber dans les données     
 
    C=initPlusPlus(X,K)
        
    while iteration < Max_iterations:
        iteration +=1
        #################################################################
        # E step : estimation des données manquantes 
        #          affectation des données aux clusters les plus proches
        for k in range(K):
            Dist[k,:]=np.linalg.norm(X-C[:,k],axis=1)**2
        y=np.argmin(Dist,axis=0)
        #################################################################
        # M Step : calcul des meilleurs centres          
        for k in range(K):
              c_k=X[y==k]
              dist=np.array([sum([np.linalg.norm(c_k[j]-c_k[i])**2 for i in range(len(c_k))]) for j in range(len(c_k))])
              C[:,k]=c_k[np.argmin(dist)]
        #################################################################
        # test du critère d'arrêt l'évolution du critère est inférieure 
        # au Seuil en pour ceent
        J[iteration]=np.sum(np.min(Dist[y,:],axis=0))/N

        if np.abs(J[iteration]-J[iteration-1])/J[iteration-1]<Seuil:
             break;
    return C, y,J[1:iteration]





def visalisationn(X,methode,K):
    colors =['r','b','g','c','m','o']
    n_colors = 6
    # we only take the first two features.
    # y = iris.target
    # fig = plt.figure(2, figsize=(8, 6))
    # plt.clf()
    # plt.scatter(X[0:50, 0], X[0:50, 1], cmap=plt.cm.Set1,edgecolor='k',label=iris.target_names[0])
    # plt.scatter(X[50:100, 0], X[50:100, 1], cmap=plt.cm.Set1,edgecolor='k',label=iris.target_names[1])
    # plt.scatter(X[100:150, 0], X[100:150, 1], cmap=plt.cm.Set1,edgecolor='k',label=iris.target_names[2])
    
    # plt.xlabel('Sepal length')
    # plt.ylabel('Sepal width')
    # plt.legend(scatterpoints=1)


    Cluster, y, Critere = methode(X,K,Visualisation = False)
    
    # Cluster, y, Critere = my_kmeans(iris.data,K,Visualisation = False)
    
    fig = plt.figure(3, figsize=(18, 5))
    plt.subplot(1,3,1)
    for k in range(K):
        plt.plot(X[y==k, 0], X[y==k, 1], colors[k%n_colors]+'o')
    plt.plot(Cluster[0, :], Cluster[1,:],'kx')
    plt.title('K moyennes ('+str(K)+')')

    plt.subplot(1,3,2)
    plt.plot(Critere, 'o-')
    plt.xlabel('Iteration')
    plt.ylabel('Mean Squared Error (MSE)')
    plt.title('Evolution du critère')
    plt.subplot(1,3,3)
    q=np.array([qualite_regroupement(X,my_kmeans,k) for k in range(1,10)])
    plt.xlabel('Nombre de groupe')
    plt.ylabel('Pourcentage de variance totale expliquée ')
    plt.title('Qualité de regroupement de k-mean-plus-plus')
    plt.plot([i[1] for i in q],[i[0] for i in q])
    plt.suptitle("K-moyennes sur le dataset IRIS k=3")
    plt.savefig('qualite')
    plt.show()





def qualite_regroupement(X,methode,k):
    Cluster, y, Critere = methode(X,k,Visualisation = False)
    Ik=np.array([np.sum((X[y==k]-Cluster[:,k])**2)/len(X[y==k]) for k in range(len(Cluster[0]))])
    Iw=np.sum([(len(X[y==k])*Ik[k])/len(X) for k in range(k)])
    Ib=np.sum([(len(X[y==k])/len(X))*(X.mean(axis=0)-Cluster[:,k])**2 for k in range(k)])
    It=Iw+Ib
    C=100*(1-(Iw/It))
    return C,k


iris = datasets.load_iris()
X=iris.data
Y=datasets.load_breast_cancer(return_X_y=False, as_frame=False)['data'][:,:2]


qualite_regroupement(X,my_kmeans,3)


qualite_regroupement(X,my_kmeans_plus_plus,3)


qualite_regroupement(X,my_kmean_medoide,9)


visalisationn(X,my_kmean_medoide,3)


q=np.array([qualite_regroupement(X,my_kmeans,k) for k in range(1,10)])
fig = plt.figure(figsize=(8, 6))
plt.xlabel('Nombre de groupe')
plt.ylabel('Pourcentage de variance totale expliquée ')
plt.title('Qualité de regroupement de k-mean-plus-plus')
plt.plot([i[1] for i in q],[i[0] for i in q])
plt.savefig('qualite')
plt.show()


from sklearn.cluster import KMeans
# calcul des k-means

# les centroïdes résultas







qualite_regroupement(X,my_kmeans,3)


conda install -c conda-forge scikit-learn-extra


pip install https://github.com/scikit-learn-contrib/scikit-learn-extra/archive/master.zip


pip install https://github.com/scikit-learn-contrib/scikit-learn-extra/archive/master.zip


pip install --upgrade pip


iris = datasets.load_iris()
X=iris.data
Y=datasets.load_breast_cancer(return_X_y=False, as_frame=False)['data'][:,:2]


from sklearn_extra.cluster import KMedoids 
colors =['r','b','g','c','m','o']
n_colors = 6
K=3
# we only take the first two features.
# y = iris.target
# fig = plt.figure(2, figsize=(8, 6))
# plt.clf()
# plt.scatter(X[0:50, 0], X[0:50, 1], cmap=plt.cm.Set1,edgecolor='k',label=iris.target_names[0])
# plt.scatter(X[50:100, 0], X[50:100, 1], cmap=plt.cm.Set1,edgecolor='k',label=iris.target_names[1])
# plt.scatter(X[100:150, 0], X[100:150, 1], cmap=plt.cm.Set1,edgecolor='k',label=iris.target_names[2])

# plt.xlabel('Sepal length')
# plt.ylabel('Sepal width')
# plt.legend(scatterpoints=1)


Cluster1, y1, Critere = my_kmeans_plus_plus(X,3,Visualisation = False)
kmeans = KMedoids(n_clusters=3, random_state=0).fit(X)
y1=kmeans.labels_
c1=kmeans.cluster_centers_.T
Cluster2, y2, Critere = my_kmean_medoide(Y,3,Visualisation = False)
kmeans = KMedoids(n_clusters=3, random_state=0).fit(Y)
y2=kmeans.labels_
c2=kmeans.cluster_centers_.T
# Cluster, y, Critere = my_kmeans(iris.data,K,Visualisation = False)

fig = plt.figure(3, figsize=(12, 8))
plt.subplot(2,2,1)
for k in range(K):
    plt.plot(X[y1==k, 0], X[y1==k, 1], colors[k%n_colors]+'o')
plt.plot(Cluster1[0, :], Cluster1[1, :],'kx')
plt.title('my_Kmedoide sur iris k= ('+str(K)+')')

plt.subplot(2,2,2)
for k in range(K):
    plt.plot(X[y1==k, 0], X[y1==k, 1], colors[k%n_colors]+'o')
plt.plot(c1[0, :], c1[1, :],'kx')
plt.title('K-medoide de scikit learn sur le data set iris k= ('+str(K)+')')
plt.subplot(2,2,3)
for k in range(K):
    plt.plot(Y[y2==k, 0], Y[y2==k, 1], colors[k%n_colors]+'o')
plt.plot(Cluster2[0, :], Cluster2[1, :],'kx')
plt.title('my_Kmedoide sur Breast cancer k= ('+str(K)+')')

plt.subplot(2,2,4)
for k in range(K):
    plt.plot(Y[y2==k, 0], Y[y2==k, 1], colors[k%n_colors]+'o')
plt.plot(c2[0, :], c2[1, :],'kx')
plt.title('K-medoide de scikit learn sur le data set Breast cancer k= ('+str(K)+')')
plt.suptitle("Comparaison de my_kmedoide et k-medoide de sckitlearn sur les deux datasets avec k=3")
plt.savefig('qualite')
plt.show()


Cluster


c1


kmeans = KMeans(n_clusters=3,init='random',n_init = 10, verbose=1, max_iter=100).fit(X)
y1=kmeans.labels_
Cluster1=kmeans.cluster_centers_
# Cluster, y, Critere = my_kmeans(iris.data,K,Visualisation = False)

fig = plt.figure(3, figsize=(12, 6))
for k in range(K):
    plt.plot(X[y1==k, 0], X[y1==k, 1], colors[k%n_colors]+'o')
plt.plot(Cluster1[0, :], Cluster1[1, :],'kx')
plt.title('my_Kmean sur iris k= ('+str(K)+')')


Cluster1



